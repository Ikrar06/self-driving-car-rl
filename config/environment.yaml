# ============================================================================
# Environment Configuration
# Shared settings for all training scripts (DQN, PPO, comparison, etc.)
# ============================================================================

# ============================================================================
# WINDOW SETTINGS
# ============================================================================
window:
  width: 1000    # Window width in pixels
  height: 700    # Window height in pixels
  fps: 60        # Target frames per second

# ============================================================================
# CAR PHYSICS
# Agent learns FULL control: steering, speed, braking (no assists!)
# Smooth steering: car turns gradually like real car (not instant snapping)
# ============================================================================
car:
  # Dimensions
  width: 30      # Car width in pixels (smaller = easier maneuvering)
  height: 15     # Car height in pixels

  # Velocity limits
  max_velocity: 50.0   # Maximum forward speed (pixels/frame)
  min_velocity: -1.0   # Maximum reverse speed (slower backing)

  # Dynamics
  acceleration: 0.25   # Acceleration rate per frame
  friction: 0.95       # Friction coefficient (0.0=no friction, 1.0=no slowdown)
  turn_rate: 0.08      # Angular velocity change per frame (radians)
  angular_damping: 0.85  # Smooth steering damping (0.0=instant turn, 1.0=no damping)

# ============================================================================
# SENSORS (Ray-casting for obstacle detection)
# ============================================================================
sensors:
  num_sensors: 7    # Number of ray-cast sensors
  angles: [-90, -60, -30, 0, 30, 60, 90]  # Angles in degrees relative to car heading
  max_range: 350    # Maximum detection range in pixels
  render_rays: true # Visualize sensor rays during training

# ============================================================================
# REWARDS (Optimized and balanced for effective learning)
# Additional reward shaping handled in simulation.py:
#   - Wall proximity penalty (graduated based on distance)
#   - Turning rewards when sensors detect danger
#   - Center of track bonus
#   - Speed bonus (only when safe)
#   - Distance traveled reward
# ============================================================================
rewards:
  survival: 2.0      # Per-step reward for staying alive (encourages longer episodes)
  checkpoint: 500.0  # Large reward for reaching checkpoints (main progress signal)
  crash: -2000.0     # Penalty for crashing (strong negative signal)
  finish: 1000.0     # Bonus for completing full lap

# ============================================================================
# FINISH TIME BONUS (Encourages faster lap completion)
# ============================================================================
finish_time:
  enabled: true           # Enable/disable time-based rewards
  mode: "speed_based"     # Mode: "speed_based" or "target_based"

  # ----------------------------------------------------------------------------
  # SPEED_BASED MODE (Recommended)
  # Faster finish = more reward
  # Formula: reward = (max_time - actual_time) × speed_multiplier
  # Example: max=80s, finish in 25s → bonus = (80-25) × 30 = 1650
  # ----------------------------------------------------------------------------
  max_time: 80.0          # Maximum expected lap time in seconds (baseline)
  speed_multiplier: 30.0  # Reward per second saved below max_time

  # ----------------------------------------------------------------------------
  # TARGET_BASED MODE (Alternative)
  # Has target time: faster=bonus, slower=penalty
  # Formula: if time < target → +(target-time) × bonus_mult
  #          else            → -(time-target) × penalty_mult
  # ----------------------------------------------------------------------------
  target_time: 30.0       # Target lap time in seconds (ideal finish time)
  bonus_multiplier: 50.0  # Bonus per second under target
  penalty_multiplier: 10.0  # Penalty per second over target

# ============================================================================
# EPISODE SETTINGS
# ============================================================================
episode:
  max_steps: 5000           # Maximum steps per episode before timeout
  start_position: [150, 250]  # Default start position [x, y] in pixels
  start_angle: 0            # Starting angle in radians (0 = facing right)

# ============================================================================
# RENDERING (Visualization settings)
# ============================================================================
rendering:
  show_sensors: true        # Display sensor rays
  show_checkpoints: true    # Display checkpoint lines
  show_info_panel: true     # Display episode info overlay
  show_trail: true          # Display car's movement trail
  trail_length: 50          # Number of previous positions to show
  trail_color: [100, 100, 255]     # RGB color for trail (blue)
  sensor_color: [0, 255, 0]        # RGB color for sensor rays (green)
  sensor_hit_color: [255, 0, 0]    # RGB color when sensor hits wall (red)

# ============================================================================
# PHYSICS (Simulation timing and realism)
# ============================================================================
physics:
  dt: 0.016               # Time step in seconds (1/60 for 60 FPS)
  use_realistic_physics: true  # Use realistic car physics vs arcade-style

# ============================================================================
# STATE SPACE (What the agent observes)
# State vector = [sensor_1, ..., sensor_7, velocity]
# - Sensors: normalized distance readings [0, 1] (0=max range, 1=touching wall)
# - Velocity: normalized speed [0, 1] based on min/max velocity
# Total state dimension: 8 (7 sensors + 1 velocity)
# ============================================================================
state:
  normalize_state: true     # Normalize all state values to [0, 1]
  include_position: false   # Add (x, y) to state (for debugging only)
  include_angle: false      # Add heading angle to state (for debugging only)

# ============================================================================
# ACTION SPACE (What the agent can do)
# 9 discrete actions = 3 steering directions × 3 speed levels
# Agent learns when to turn, how much, and what speed to use
# ============================================================================
actions:
  type: "discrete"    # Action type: "discrete" or "continuous"
  num_actions: 9      # Total number of discrete actions

  # Action mapping (3 steering × 3 speed = 9 combinations)
  discrete_actions:
    - "LEFT_SLOW"       # 0: Turn left  + slow speed
    - "LEFT_NORMAL"     # 1: Turn left  + normal speed
    - "LEFT_FAST"       # 2: Turn left  + fast speed
    - "STRAIGHT_SLOW"   # 3: Straight   + slow speed
    - "STRAIGHT_NORMAL" # 4: Straight   + normal speed
    - "STRAIGHT_FAST"   # 5: Straight   + fast speed
    - "RIGHT_SLOW"      # 6: Turn right + slow speed
    - "RIGHT_NORMAL"    # 7: Turn right + normal speed
    - "RIGHT_FAST"      # 8: Turn right + fast speed
